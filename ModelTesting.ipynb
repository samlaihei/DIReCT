{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import ehtim as eh\n",
    "\n",
    "from torchvision.transforms import v2\n",
    "import data.dataset_img as ds\n",
    "import torchvision\n",
    "import model_AttnAE2 as mlmodel\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "import glob\n",
    "\n",
    "from xlogger import *\n",
    "\n",
    "# use gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = 'AttnAE_all_v3'\n",
    "append = True\n",
    "train_logger = xlogger('models/history/' + model_name + '_train.dat', append=append)\n",
    "val_logger = xlogger('models/history/' + model_name + '_val.dat', append=append)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all images in the batch\n",
    "def plot_images(images, save=False, name='images', cmap='viridis', return_axes=False, show=True):\n",
    "    fig, axes = plt.subplots(2, len(images)//2, figsize=(len(images)//2*0.99, 2))\n",
    "    fig.subplots_adjust(hspace=0., wspace=0.)\n",
    "    axes = axes.flatten()\n",
    "    for ax, img in zip(axes, images):\n",
    "        ax.imshow(img.permute(1, 2, 0), cmap=cmap)\n",
    "        ax.axis('off')\n",
    "    if save:\n",
    "        direc = 'models/history/'+name+'/'\n",
    "        os.makedirs(direc, exist_ok=True)\n",
    "        # check next number\n",
    "        num = 0\n",
    "        while glob.glob(f'{direc}{name}_{num}.png'):\n",
    "            num += 1\n",
    "        plt.savefig(f'{direc}{name}_{num}.png')\n",
    "    if show:\n",
    "        plt.show()\n",
    "    if return_axes:\n",
    "        return axes\n",
    "\n",
    "def log_avgLoss(data, logger):\n",
    "    # log the avg loss given array of all epochs\n",
    "    data = np.sum(np.transpose(data), axis=1)/len(data)\n",
    "    kwargs = {'features_loss': data[0],\n",
    "              'class_loss': data[1],\n",
    "              'encoder_corr': data[2],\n",
    "              'pred_corr': data[3],\n",
    "              'weighted_loss': data[4]}\n",
    "    logger.write(kwargs)\n",
    "    return data\n",
    "    \n",
    "\n",
    "\n",
    "def nxcorr(outputs, labels):\n",
    "    dim = int(outputs.shape[-1])\n",
    "    outputs = outputs.reshape(-1, dim**2)\n",
    "    labels = labels.reshape(-1, dim**2)\n",
    "    \n",
    "    outputs_norm = (outputs.reshape(-1, dim, dim) - torch.nanmean(outputs, axis=1).reshape(-1, 1, 1)) / torch.std(outputs, axis=1).reshape(-1, 1, 1)\n",
    "    labels_norm = (labels.reshape(-1, dim, dim) - torch.nanmean(labels, axis=1).reshape(-1, 1, 1)) / torch.std(labels, axis=1).reshape(-1, 1, 1)\n",
    "\n",
    "    fft_outputs = torch.fft.fftn(outputs_norm, s=[outputs_norm.size(d)*1 for d in [1,2]], dim=[1,2])\n",
    "    fft_labels = torch.fft.fftn(labels_norm, s=[outputs_norm.size(d)*1 for d in [1,2]], dim=[1,2])\n",
    "\n",
    "    xcorr = torch.fft.ifftn(fft_outputs * torch.conj(fft_labels), dim=[1,2])\n",
    "\n",
    "    nxcorr_flat = xcorr.reshape(-1, dim**2)\n",
    "    idx = torch.argmax(torch.abs(nxcorr_flat), dim=1)\n",
    "\n",
    "    return idx, torch.abs(nxcorr_flat[torch.arange(nxcorr_flat.shape[0]), idx])/dim**2\n",
    "\n",
    "def shift_image(im1, im2): # shift single im2 by idx\n",
    "    idx, _ = nxcorr(im1, im2)\n",
    "    im2 = torch.roll(im2, shifts=int(idx))\n",
    "    return im1, im2\n",
    "\n",
    "def shift_all(truth, imgs):\n",
    "    shifted_imgs = []\n",
    "    for img in imgs:\n",
    "        _, shifted_img = shift_image(truth, img)\n",
    "        shifted_imgs.append(shifted_img)\n",
    "    return np.array(shifted_imgs)\n",
    "\n",
    "def nxcorr_loss(outputs, labels): # assume square images\n",
    "    _, xcorr = nxcorr(outputs, labels)\n",
    "    loss_mean = torch.nanmean(1-xcorr)\n",
    "    return loss_mean\n",
    "\n",
    "def nxcorr_network_loss(out_img, label_img):\n",
    "    # normalise outputs and labels\n",
    "    # outputs = (outputs - torch.mean(outputs, axis=1).reshape(-1, 1)) / torch.std(outputs, axis=1).reshape(-1, 1)\n",
    "    # labels = (labels - torch.mean(labels, axis=1).reshape(-1, 1)) / torch.std(labels, axis=1).reshape(-1, 1)\n",
    "    mseloss = nn.MSELoss()\n",
    "    mse_weight = 1\n",
    "    mseloss_val = mseloss(out_img, label_img)\n",
    "    nxcorr_val = nxcorr_loss(out_img, label_img)\n",
    "    return mse_weight*mseloss_val + nxcorr_val*(1-mse_weight)\n",
    "\n",
    "\n",
    "mseloss = nn.MSELoss()\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "# loss_weights = torch.tensor([1, 0.03, 2.0, 3.0])\n",
    "loss_weights = torch.tensor([1.0, 0, 2.0, 3.0])\n",
    "\n",
    "def total_loss(outputs, input_imgs, classes, weights=torch.tensor([1., 1., 1., 1.]), print_loss=False, logger=None):\n",
    "    features_vae, features_q, features_ci, recon_img, pred_img, pred_class = outputs\n",
    "    # features_vae = features_vae.view(features_vae.shape[0], -1)\n",
    "    # features_ci = features_ci.view(features_ci.shape[0], -1)\n",
    "    \n",
    "    features_loss = mseloss(features_ci, features_q)\n",
    "\n",
    "    class_loss = ce_loss(pred_class, classes)\n",
    "    \n",
    "    # first channel only\n",
    "    recon_img = recon_img[:,0]\n",
    "    pred_img = pred_img[:,0]\n",
    "    input_imgs = input_imgs[:,0]\n",
    "    encoder_corr = nxcorr_network_loss(recon_img, input_imgs)\n",
    "    pred_corr = nxcorr_network_loss(pred_img, input_imgs)\n",
    "\n",
    "\n",
    "    total = weights[0]*features_loss + weights[1]*class_loss + weights[2]*encoder_corr + weights[3]*pred_corr\n",
    "\n",
    "    losses_individual = [features_loss.cpu().detach(), \n",
    "                         class_loss.cpu().detach(), \n",
    "                         encoder_corr.cpu().detach(), \n",
    "                         pred_corr.cpu().detach(), \n",
    "                         total.cpu().detach()]\n",
    "    losses_weighted = np.array([weights[i].cpu().detach()*losses_individual[i].cpu().detach() for i in range(len(weights))] + [total.cpu().detach()])\n",
    "    \n",
    "    if print_loss:\n",
    "        print(f'features_loss: {features_loss}, class_loss: {class_loss}, encoder_corr: {encoder_corr}, pred_corr: {pred_corr}')\n",
    "    if logger:\n",
    "        kwargs = {'features_loss': features_loss.cpu().detach().numpy(), \n",
    "                  'class_loss': class_loss.cpu().detach().numpy(), \n",
    "                  'encoder_corr': encoder_corr.cpu().detach().numpy(), \n",
    "                  'pred_corr': pred_corr.cpu().detach().numpy(),\n",
    "                  'weighted_loss': total.cpu().detach().numpy()}\n",
    "        logger.write(kwargs)\n",
    "        \n",
    "\n",
    "    return total, losses_individual, losses_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss_noise_tensor(img):\n",
    "    assert isinstance(img, torch.Tensor)\n",
    "    dtype = img.dtype\n",
    "    if not img.is_floating_point():\n",
    "        img = img.to(torch.float32)\n",
    "    \n",
    "    sigma = 0.1\n",
    "    \n",
    "    out = img + sigma * torch.randn_like(img)\n",
    "    \n",
    "    if out.dtype != dtype:\n",
    "        out = out.to(dtype)\n",
    "        \n",
    "    return out\n",
    "\n",
    "def square_taper(img, square_size_range=(32, 48), blur_fwhm_range=(5, 10), fixed=False):\n",
    "    assert isinstance(img, torch.Tensor)\n",
    "    dtype = img.dtype\n",
    "    if not img.is_floating_point():\n",
    "        img = img.to(torch.float32)\n",
    "    \n",
    "    h, w = img.shape[-2:]\n",
    "    if fixed:\n",
    "        square_size = square_size_range\n",
    "        blur_fwhm = blur_fwhm_range\n",
    "    else:\n",
    "        square_size = np.random.randint(*square_size_range)\n",
    "        blur_fwhm = np.random.uniform(*blur_fwhm_range)\n",
    "\n",
    "    # taper mask\n",
    "    taper = torch.zeros_like(img)\n",
    "    taper[:, h//2-square_size//2:h//2+square_size//2, w//2-square_size//2:w//2+square_size//2] = 1\n",
    "    taper = v2.GaussianBlur(25, sigma=blur_fwhm)(taper)\n",
    "    \n",
    "    out = img * taper\n",
    "    \n",
    "    if out.dtype != dtype:\n",
    "        out = out.to(dtype)\n",
    "        \n",
    "    return out\n",
    "\n",
    "def radial_taper(img, radii_range=(8, 24), blur_fwhm_range=(5, 10), fixed=False):\n",
    "    assert isinstance(img, torch.Tensor)\n",
    "    dtype = img.dtype\n",
    "    if not img.is_floating_point():\n",
    "        img = img.to(torch.float32)\n",
    "    \n",
    "    h, w = img.shape[-2:]\n",
    "    if fixed:\n",
    "        radius = radii_range\n",
    "        blur_fwhm = blur_fwhm_range\n",
    "    else:\n",
    "        radius = np.random.randint(*radii_range)\n",
    "        blur_fwhm = np.random.uniform(*blur_fwhm_range)\n",
    "\n",
    "    # radial taper mask\n",
    "    taper = torch.zeros_like(img)\n",
    "    y, x = torch.meshgrid(torch.arange(h), torch.arange(w))\n",
    "    y = y - h//2\n",
    "    x = x - w//2\n",
    "    r = torch.sqrt(x**2 + y**2)\n",
    "    taper[:, r<radius] = 1\n",
    "    taper = v2.GaussianBlur(15, sigma=blur_fwhm)(taper)\n",
    "    \n",
    "    out = img * taper\n",
    "    \n",
    "    if out.dtype != dtype:\n",
    "        out = out.to(dtype)\n",
    "        \n",
    "    return out\n",
    "\n",
    "def fixed_taper(img, size = 16, blur_fwhm = 5):\n",
    "    assert isinstance(img, torch.Tensor)\n",
    "    dtype = img.dtype\n",
    "    if not img.is_floating_point():\n",
    "        img = img.to(torch.float32)\n",
    "\n",
    "    out = radial_taper(img, size, blur_fwhm, fixed=True)\n",
    "    \n",
    "\n",
    "    if out.dtype != dtype:\n",
    "        out = out.to(dtype)\n",
    "        \n",
    "    return out\n",
    "\n",
    "\n",
    "# random resizing during training\n",
    "# albumentation\n",
    "# kornia\n",
    "# affine transformation (zoom in zoom out, rotate, shear) elastic deformation/warping\n",
    "\n",
    "train_transforms = v2.Compose([\n",
    "    v2.ToTensor(),\n",
    "    v2.RandomHorizontalFlip(),\n",
    "    v2.RandomVerticalFlip(),\n",
    "    v2.RandomAffine(degrees=(-90, 90), scale=(0.8, 1.), shear=(-5, 5), interpolation=torchvision.transforms.InterpolationMode.BILINEAR),\n",
    "    v2.CenterCrop(64),\n",
    "])\n",
    "\n",
    "val_transforms = v2.Compose([\n",
    "    v2.ToTensor(),\n",
    "    v2.CenterCrop(64),\n",
    "])\n",
    "\n",
    "val_transforms_rotate = v2.Compose([\n",
    "    v2.ToTensor(),\n",
    "    v2.RandomHorizontalFlip(),\n",
    "    v2.RandomVerticalFlip(),\n",
    "    v2.RandomAffine(degrees=(-90, 90), interpolation=torchvision.transforms.InterpolationMode.BILINEAR),\n",
    "    v2.CenterCrop(64),\n",
    "])\n",
    "\n",
    "\n",
    "val_transforms_taper = v2.Compose([\n",
    "    v2.ToTensor(),\n",
    "    fixed_taper,\n",
    "    v2.CenterCrop(64),\n",
    "])\n",
    "\n",
    "\n",
    "cifar_train_transforms = v2.Compose([\n",
    "    v2.ToTensor(),\n",
    "    v2.Resize(64),\n",
    "    v2.RandomHorizontalFlip(),\n",
    "    v2.RandomVerticalFlip(),\n",
    "    v2.RandomAffine(degrees=(-90, 90), scale=(0.8, 1.), shear=(-5, 5), interpolation=torchvision.transforms.InterpolationMode.BILINEAR),\n",
    "    radial_taper,\n",
    "    v2.CenterCrop(64),\n",
    "])\n",
    "\n",
    "cifar_val_transforms = v2.Compose([\n",
    "    v2.ToTensor(),\n",
    "    v2.Resize(64),\n",
    "    fixed_taper,\n",
    "    v2.CenterCrop(64),\n",
    "])\n",
    "\n",
    "\n",
    "test_transforms = v2.Compose([\n",
    "    v2.ToTensor(),\n",
    "    # v2.Resize(38),\n",
    "    v2.RandomHorizontalFlip(),\n",
    "    v2.RandomVerticalFlip(),\n",
    "    v2.RandomAffine(degrees=(-90, 90), scale=(0.4, 0.7), interpolation=torchvision.transforms.InterpolationMode.BILINEAR),\n",
    "    # v2.RandomAffine(degrees=(0, 0), scale=(0.62, 0.62), interpolation=torchvision.transforms.InterpolationMode.BILINEAR),\n",
    "    # radial_taper,\n",
    "    v2.CenterCrop(64),\n",
    "])\n",
    "\n",
    "test_transforms_noRotate = v2.Compose([\n",
    "    v2.ToTensor(),\n",
    "    # v2.Resize(38),\n",
    "    v2.RandomAffine(degrees=(0, 0), scale=(0.4, 0.7), interpolation=torchvision.transforms.InterpolationMode.BILINEAR),\n",
    "    # v2.RandomAffine(degrees=(0, 0), scale=(0.62, 0.62), interpolation=torchvision.transforms.InterpolationMode.BILINEAR),\n",
    "    # radial_taper,\n",
    "    v2.CenterCrop(64),\n",
    "])\n",
    "\n",
    "\n",
    "test_transforms_noise = v2.Compose([\n",
    "    v2.ToTensor(),\n",
    "    v2.RandomHorizontalFlip(),\n",
    "    v2.RandomVerticalFlip(),\n",
    "    v2.RandomAffine(degrees=(-90, 90), scale=(0.4, 0.7), interpolation=torchvision.transforms.InterpolationMode.BILINEAR),\n",
    "    v2.CenterCrop(64),\n",
    "    gauss_noise_tensor,\n",
    "])\n",
    "\n",
    "fidelity_test = v2.Compose([\n",
    "    v2.ToTensor(),\n",
    "    v2.RandomHorizontalFlip(),\n",
    "    v2.RandomVerticalFlip(),\n",
    "    v2.RandomAffine(degrees=(-90, 90), scale=(0.7, 1.3), interpolation=torchvision.transforms.InterpolationMode.BILINEAR),\n",
    "    v2.CenterCrop(64),\n",
    "])\n",
    "\n",
    "fidelity_test_shear = v2.Compose([\n",
    "    v2.ToTensor(),\n",
    "    v2.RandomHorizontalFlip(),\n",
    "    v2.RandomVerticalFlip(),\n",
    "    v2.RandomAffine(degrees=(-90, 90), scale=(0.7, 1.3), shear=(-30, 30), interpolation=torchvision.transforms.InterpolationMode.BILINEAR),\n",
    "    v2.CenterCrop(64),\n",
    "])\n",
    "\n",
    "\n",
    "transforms_dict = {\n",
    "    'train': train_transforms,\n",
    "    'val': val_transforms,\n",
    "    'cifar_train': cifar_train_transforms,\n",
    "    'cifar_val': cifar_val_transforms,\n",
    "    'test': test_transforms,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = True\n",
    "\n",
    "# initialise weights\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "if load:\n",
    "    model = torch.load('models/'+model_name+'.pt').to(device)\n",
    "else: \n",
    "    model = mlmodel.Vae_FD(k_classes=8).to(device)\n",
    "    model.apply(weights_init)\n",
    "\n",
    "if False: # WARNING: Replaces all parameters but the transformer!\n",
    "    # load encoder, decoder, classifier from pre-trained model\n",
    "    pretrained = torch.load('models/AttnAE_all.pt').named_parameters()\n",
    "    dict_params = dict(pretrained)\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'ci_xtrans' in name:\n",
    "            continue\n",
    "        if name in dict_params:\n",
    "            param.data = dict_params[name].data\n",
    "\n",
    "ehtim=True\n",
    "tint_sec = 5   \n",
    "tadv_sec = 600\n",
    "tstart_hr = 0\n",
    "tstop_hr = 24\n",
    "# psize = 7.757018897750619e-12 * 2\n",
    "psize = 1.7044214966184275e-11\n",
    "bw_hz = [230E9]#, 345E9]\n",
    "reorder = True\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "data_dir = 'data/datasets/v3/'\n",
    "mnames = ['gauss', 'disk', 'ellipse', 'ring', 'mring', 'disk2', 'gauss2', 'cifar10-128']\n",
    "\n",
    "val_data = ds.ImgDataset(['data/datasets/val/val_mring.npy'], transform=val_transforms, ehtim=ehtim,\n",
    "                            tint_sec=tint_sec, tadv_sec=tadv_sec, tstart_hr=tstart_hr, tstop_hr=tstop_hr, bw_hz=bw_hz, psize=psize,\n",
    "                            )\n",
    "\n",
    "val = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dataiter = iter(val)\n",
    "static_val = next(dataiter)\n",
    "\n",
    "# plot_images(static_val[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model\n",
    "test = DataLoader(val_data, batch_size=32, shuffle=True)\n",
    "for i, data in enumerate(test, 0):\n",
    "    img, ci, cls = data\n",
    "    ci = torch.tensor(val.dataset.closure.FTCI(img).reshape(batch_size, -1), dtype=torch.float32)\n",
    "    img, ci, cls = img.to(device), ci.to(device), cls.to(device)\n",
    "    features_vae, features_q, features_ci, recon_img, pred_img, pred_class = model(img, ci)\n",
    "    plot_images(img.cpu().detach())\n",
    "    plot_images(recon_img.cpu().detach()) \n",
    "    plot_images(pred_img.cpu().detach()) \n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing with image\n",
    "import seaborn as sns\n",
    "\n",
    "cmap = 'Greys'\n",
    "\n",
    "def gauss_noise(img, sigma):\n",
    "    out = img + sigma * torch.randn_like(img).numpy()\n",
    "    return out\n",
    "\n",
    "def N_noisy_reconstructions(N, img, stds, add_th_noise=False):\n",
    "    outs = []\n",
    "    test_ims = []\n",
    "    for std in stds:\n",
    "        batch_imgs = img.repeat(N, 1, 1)\n",
    "        test_im = gauss_noise(batch_imgs, std).reshape(N, imgdim, imgdim)\n",
    "        test_ims.append(test_im)\n",
    "\n",
    "    test_ims = np.array(test_ims).reshape(-1, imgdim, imgdim)\n",
    "    test_ims = torch.tensor(test_ims, dtype=torch.float32).to(device)\n",
    "    test_invs = val.dataset.closure.FTCI(test_ims, add_th_noise=add_th_noise).reshape(test_ims.shape[0], -1)\n",
    "    test_invs = torch.tensor(test_invs, dtype=torch.float32).reshape(test_ims.shape[0], -1)\n",
    "    test_invs = test_invs.to(device)\n",
    "\n",
    "    for inv in test_invs:\n",
    "        _, outputs, _ = model.predict_with_attn(inv.reshape(1, -1))\n",
    "        outputs = outputs.cpu().detach().numpy().reshape(imgdim,imgdim)\n",
    "        outputs = (outputs - np.min(outputs))/(np.max(outputs) - np.min(outputs))\n",
    "        outs.append(outputs)\n",
    "    \n",
    "    return np.array(outs)\n",
    "\n",
    "def N_transforms(N, img, transform, add_th_noise=False):\n",
    "    # fidelity distribution\n",
    "    fides = []\n",
    "    outs = []\n",
    "    test_im = img.repeat(N, 1, 1)\n",
    "    \n",
    "    test_im = torch.tensor(np.array([transform(i.reshape(1, imgdim, imgdim)) for i in test_im]), dtype=torch.float32).to(device)\n",
    "    test_inv = val.dataset.closure.FTCI(test_im, add_th_noise=add_th_noise).reshape(N, -1)\n",
    "    test_inv = torch.tensor(test_inv, dtype=torch.float32).to(device)\n",
    "\n",
    "    for img, inv in zip(test_im, test_inv):\n",
    "        _, outputs, _ = model.predict_with_attn(inv)\n",
    "        outputs = outputs.cpu().detach().numpy().reshape(imgdim,imgdim)\n",
    "        outputs = (outputs - np.min(outputs))/(np.max(outputs) - np.min(outputs))\n",
    "        fide = nxcorr(torch.Tensor(outputs).to(device), torch.Tensor(img))[1].cpu().detach().numpy()[0]\n",
    "        fides.append(fide)\n",
    "        outs.append(outputs)\n",
    "\n",
    "    return test_im, test_inv, outs, fides\n",
    "\n",
    "def ecdf(a):\n",
    "    x, counts = np.unique(a, return_counts=True)\n",
    "    cusum = np.cumsum(counts)\n",
    "    return x, cusum / cusum[-1]\n",
    "\n",
    "def crps_score(truth, imgs):\n",
    "    imgs = np.array(imgs)\n",
    "    imgs = imgs.reshape(imgs.shape[0], -1)\n",
    "    truth = truth.reshape(-1)\n",
    "    crps_scores = []\n",
    "    for i in range(imgs.shape[1]):\n",
    "        out_pix = imgs[:, i]\n",
    "        mean_out_pix = np.mean(out_pix)+0.1\n",
    "        truth_pix = truth[i]\n",
    "        x, y = ecdf(out_pix)\n",
    "        step_fc = np.heaviside(x-truth_pix, 0.5)\n",
    "        crps = np.trapz((y-step_fc)**2, x)\n",
    "        crps_scores.append(crps)\n",
    "\n",
    "    crps_scores = np.array(crps_scores).reshape(64, 64)\n",
    "    tot_crps = np.sum(crps_scores)/(truth.shape[0])\n",
    "    return tot_crps, crps_scores    \n",
    "\n",
    "imgdim = 64\n",
    "\n",
    "model = torch.load('models/'+model_name+'.pt')\n",
    "\n",
    "load_fits = 'Images/sgra.fits'\n",
    "img = eh.image.load_fits(load_fits)\n",
    "name = load_fits.split('/')[-1].split('.')[0]\n",
    "\n",
    "# im = img.regrid_image(img.fovx(), imgdim).imarr()\n",
    "im = img.imarr()\n",
    "\n",
    "im = im/np.max(im) # normalise the intensity\n",
    "# im = test_transforms(im)\n",
    "# im = test_transforms_noRotate(im)\n",
    "# im = val_transforms(im)\n",
    "im = fidelity_test_shear(im)\n",
    "im = torch.tensor(im).to(torch.float32).reshape(1, imgdim, imgdim)\n",
    "# im = gauss_noise(im, 0.1).reshape(1, imgdim, imgdim)\n",
    "in_im = np.copy(im)\n",
    "\n",
    "img._imdict['I'] = np.swapaxes(in_im, 1, 2) # swap img axis for obs\n",
    "img.mjd = 57848\n",
    "img.ra = 187.7059167/360*24\n",
    "img.dec = 12.3911222\n",
    "img.psize = 1.7044214966184275e-11 \n",
    "\n",
    "integrated_flux = 100\n",
    "\n",
    "img._imdict['I'] = img._imdict['I']/img.total_flux()*integrated_flux\n",
    "\n",
    "obs = img.observe(eh.array.load_txt('data/EHT2017.txt'), tint_sec, tadv_sec, tstart_hr, tstop_hr, 8e9, mjd = img.mjd, ttype='DFT',  \n",
    "            sgrscat=False, ampcal=True, phasecal=True, noise=True, add_th_noise=True, th_noise_factor=1,\n",
    "            gainp=0.1, verbose=False)\n",
    "\n",
    "img._imdict['I'] = np.swapaxes(in_im, 1, 2) # swap back after obs\n",
    "\n",
    "\n",
    "if True:\n",
    "    invs = torch.tensor(val.dataset.closure.FTCI(im, add_th_noise=False, method=1, th_noise_factor=1).reshape(1, -1), dtype=torch.float32).to(device)\n",
    "else:\n",
    "    invs = torch.tensor(obs.ClosureInvariants().reshape(1, -1), dtype=torch.float32).to(device) # invs from obs\n",
    "\n",
    "\n",
    "\n",
    "out_features, outputs, attns = model.predict_with_attn(invs)\n",
    "outputs = outputs.cpu().detach().numpy().reshape(imgdim,imgdim)\n",
    "outputs = (outputs - np.min(outputs))/(np.max(outputs) - np.min(outputs))\n",
    "\n",
    "im, outputs = im.reshape(imgdim, imgdim), outputs.reshape(imgdim, imgdim)#.numpy()\n",
    "\n",
    "\n",
    "encoder_latent, _, encoder_out = model.encoder_to_img(torch.Tensor(im).view(1,1,imgdim, imgdim).to(device))\n",
    "encoder_out = encoder_out.cpu().detach().numpy().reshape(imgdim, imgdim)\n",
    "\n",
    "# predict class\n",
    "cls_pred = model.predict_class(invs).cpu().detach().numpy()\n",
    "print('Classes:', cls_pred)\n",
    "cls_pred = mnames[np.argmax(cls_pred)]\n",
    "print(cls_pred)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize=(15,5))\n",
    "ax[0].imshow(im, cmap=cmap)\n",
    "ax[0].set_title('Truth')\n",
    "\n",
    "ax[1].imshow(encoder_out, cmap=cmap)\n",
    "ax[1].set_title('Encoder Output')\n",
    "idx, xcorr = nxcorr(torch.tensor(encoder_out), torch.tensor(in_im))\n",
    "xcorr = xcorr.cpu().detach().numpy()[0]\n",
    "ax[1].text(0.5, 0.1, f'{xcorr:.3f}', ha='center', va='center', transform=ax[1].transAxes, color='k')\n",
    "\n",
    "ax[2].imshow(outputs, cmap=cmap)\n",
    "ax[2].set_title('DIReCT Output')\n",
    "idx, xcorr = nxcorr(torch.tensor(outputs), torch.tensor(in_im))\n",
    "xcorr = xcorr.cpu().detach().numpy()[0]\n",
    "ax[2].text(0.5, 0.1, f'{xcorr:.3f}', ha='center', va='center', transform=ax[2].transAxes, color='k')\n",
    "\n",
    "\n",
    "ax[3].imshow(im-outputs)\n",
    "ax[3].set_title('CI Residual')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Maps\n",
    "\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "\n",
    "# average over head and layers and patch (collapse)\n",
    "avg_attns = attns.mean(axis=(0,1,2,3)).cpu().detach().numpy()[:-1]\n",
    "\n",
    "# normalise\n",
    "avg_attns = avg_attns/np.max(avg_attns)\n",
    "\n",
    "ci, uv_orig, _ = val.dataset.closure.FTCI(np.array([in_im, in_im]), return_uv=True)\n",
    "uv = uv_orig[0]\n",
    "uv = np.repeat(uv, 2, axis=2)\n",
    "ci = np.array(ci.reshape(-1))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 3))\n",
    "ax.plot(avg_attns)\n",
    "\n",
    "    \n",
    "ax.set_ylabel('Normalised Attention')\n",
    "ax.set_xlabel('CI Index')\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "# plot triangles with uv\n",
    "for i in range(uv.shape[-1]):\n",
    "    attn_val = avg_attns[i]\n",
    "    ci_val = ci[i]\n",
    "    # map attn_val to colormap\n",
    "    cmap = plt.get_cmap('binary')\n",
    "    attn_col = cmap(attn_val)\n",
    "\n",
    "    if attn_val > 0.6:\n",
    "        # print(uv[:, :, i].T)\n",
    "        pass\n",
    "\n",
    "    s = Polygon(np.array(uv[:, :, i].T), edgecolor=attn_col, facecolor=attn_col, alpha=attn_val, linewidth=attn_val, zorder=1+attn_val)\n",
    "    ax.add_patch(s)\n",
    "\n",
    "\n",
    "# plot all uv points\n",
    "uv_plot = uv_orig.reshape(2, -1)\n",
    "print(uv_plot[0, :6])\n",
    "print(uv_plot[1, :6])\n",
    "ax.scatter(uv_plot[0, :], uv_plot[1, :], color='C0', zorder=5, s=5)\n",
    "\n",
    "ax.set_xlim(-1e10, 1e10)\n",
    "ax.set_ylim(-1e10, 1e10)\n",
    "ax.grid()\n",
    "ax.set_xlabel('u')\n",
    "ax.set_ylabel('v')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference between two attention maps\n",
    "\n",
    "img1 = eh.image.load_fits('Images/ring.fits')\n",
    "img2 = eh.image.load_fits('Images/sgra.fits')\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,5))\n",
    "imgs = [img1, img2]\n",
    "transforms = [val_transforms, test_transforms]\n",
    "attns_array = []\n",
    "\n",
    "for i, img in enumerate(imgs):\n",
    "    im = img.imarr()\n",
    "    im = im/np.max(im) # normalise the intensity\n",
    "\n",
    "    im = transforms[i](im)\n",
    "    im = im.to(torch.float32)\n",
    "    ax[i].imshow(im[0])\n",
    "\n",
    "    invs = val.dataset.closure.FTCI(im).reshape(1, -1)\n",
    "\n",
    "    invs = torch.tensor(invs, dtype=torch.float32).reshape(1, -1)\n",
    "    invs = invs.to(device)\n",
    "    out_features, outputs, attns = model.predict_with_attn(invs)\n",
    "    outputs = outputs.cpu().detach().numpy().reshape(imgdim,imgdim)\n",
    "    avg_attns = attns.mean(axis=(0,1,2,3)).cpu().detach().numpy()[:-1]\n",
    "    attns_array.append(avg_attns)\n",
    "\n",
    "\n",
    "\n",
    "diff = np.abs(attns_array[0] - attns_array[1])\n",
    "# diff = attns_array[0]/attns_array[1]\n",
    "\n",
    "\n",
    "# normalise\n",
    "diff = (diff - np.min(diff))/(np.max(diff) - np.min(diff))\n",
    "\n",
    "ci, uv_orig, _ = val.dataset.closure.FTCI(in_im, return_uv=True)\n",
    "uv = uv_orig[0]\n",
    "uv = np.repeat(uv, 2, axis=2)\n",
    "ci = np.array(ci.reshape(-1))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 3))\n",
    "ax.plot(diff)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "\n",
    "for i in range(uv.shape[-1]):\n",
    "    attn_val = diff[i]\n",
    "    # map attn_val to colormap\n",
    "    cmap = plt.get_cmap('binary')\n",
    "    attn_col = cmap(attn_val)\n",
    "\n",
    "    s = Polygon(np.array(uv[:, :, i].T), edgecolor=attn_col, facecolor=attn_col, alpha=attn_val, linewidth=attn_val, zorder=1+attn_val)\n",
    "    ax.add_patch(s)\n",
    "\n",
    "uv_plot = uv_orig.reshape(2, -1)\n",
    "ax.scatter(uv_plot[0, :], uv_plot[1, :], color='C0', zorder=5, s=5)\n",
    "\n",
    "ax.set_xlim(-1e10, 1e10)\n",
    "ax.set_ylim(-1e10, 1e10)\n",
    "ax.grid()\n",
    "ax.set_xlabel('u')\n",
    "ax.set_ylabel('v')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
